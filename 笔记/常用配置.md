#  Linux

学习网址：https://www.runoob.com/

## linux网络配置

**桥接模式：**与物理电脑主机使用同一个段位，能联网 

**NAT模式：**使用网卡 VMnet8 ，虚拟机ip由虚拟DHCP服务器分配，外网不能与此虚拟机直接通信

### NAT模式配置静态IP

```
1 通过网络适配器将连接模式改为NAT模式
2 在虚拟机终端输入 vi /etc/sysconfig/network-scripts/ifcfg-eth0
  进入文件
3 编辑文件内容
  BOOTPROTO=static     ---static|none静态配置  dhcp是自动分配
  IPADDR=192.168.108.X    --确定自己的网络段位号（与虚拟机编译器提供的网络段位一致）,x自己提供，不能使用网关和网卡使用的数字
  NETMASK=255.255.255.0  --子网掩码
  GATEWAY=192.168.108.2  --网关，查看虚拟网络编辑器提供的网关
  DNS1=192.168.108.2   --域名解析
  DNS2=8.8.8.8 
  DNS3=114.114.114.114 
4 重启网络服务 service network restart
5 查看网卡信息  ifconfig 
6 检查网络通信状态 使用ping命令
	-- 检查虚拟机能否连接外网
	 ping www.baidu.com
	-- 检查虚拟机能否与物理机通信
	  ping 物理机IP
	-- 检查物理机能否与虚拟机通信
	  ipconfig:windows系统查看网络信息
	  ping 虚拟机IP 
```

### 桥接模式配置静态IP

虚拟机IP地址需要与主机IP在同一个网段；网关和DNS需要与主机保持一致

```
3 编辑文件内容
  BOOTPROTO=static     ---static|none静态配置  dhcp是自动分配
  IPADDR=192.168.108.x   查看虚拟网络编译器提供的网络段位号，确定自													己的段位号
  NETMASK=255.255.255.0  --子网掩码
  GATEWAY=192.168.108.2  --网关，查看虚拟网络编译器提供的网关
  DNS1=192.168.108.2   ---域名解析  
  DNS2=8.8.8.8 
  DNS3=114.114.114.114 
```

## Linux用户和用户组

### 用户和用户组常用命令（root）

```
1 user :Linux系统登陆账号,对应UID存储在/etc/passwd文件中，登陆时候以UID辨别是否存在此user用户
2 geoup：用户组，linux可以有多个用户组，UID作为数字唯一表示符
------------------------------------------------------------
1 添加用户
   useradd [选项] [用户名]（此选项 12通用）
   	 -c  指定一段注释性描述。
     -d  指定用户主目录，目录不存在，则同时使用-m，可以创建主目录。
     -g  用户组指定用户所属的用户组（指定用户组必须存在否则创建失败）
     -G  用户组，指定用户所属的附加组。
     -s Shell文件 指定用户的登录Shell。
     -u 用户ID号 指定用户的UID
2 修改用户
    usermod [选项]  [用户名]
    修改用户名：
    usermod  -l  newName  -d /home/newName  oldName
    
3 修改用户密码
	passwd [选项]  [用户名]
	  -l 锁定口令，即禁用账号。
      -u 口令解锁。
      -d 使账号无口令。
      -f 强迫用户下次登录时修改口令。
4 删除用户
	userdel  用户名  ：删除用户
	userdel -r 用户名 ：home目录下的文件也会被删除
------------------------------------------------------------
1 创建用户组
	groupadd  [选项]  [用户组]
	   -g  ： 指定新用户组的组标识号（gid）。
       -o ： 一般与-g选项同时使用，表示新用户组的gid可以与系统已有用                                                户组的gid相同。
2 修改用户组
    groupmod  [选项]  新组名  原组名
      	-g： 为用户组指定新的组标识号。
        -o ：与-g选项同时使用，用户组的新gid可以与系统已有用户组的gid
                                                      相同。
        -n  将用户组的名字改为新名字
3 删除用户组
	groupdel  [用户组] 
------------------------------------------------------------
   groupadd 组名 ：创建组不会创建用户
   gpasswd -a 用户名 组名  ：将用户加到组中（附加组）
   gpasswd -d 用户名 组名 : 将用户从组中删除（附加组）
   groups 用户名 ：查看当前用户所在组
```

### sudo权限设置

**普通用户新创建的用户没有口令，账号被系统锁定无法使用，需要获得sudo权限之后用sudo passwd 用户名  为用户设置密码才可以使用****；**root创建的则不需要****

```
1 在root权限下 用visudo进入文件修改权限
	set nu :显示行号
 大概90行位置： root ALL=(ALL) ALL 在这个下面添加： 
 									用户名 ALL=(ALL) ALL
 大概100多：行位置： #%whell  ALL=(ALL)  NOPASSWD 下面添加 ：
         					用户名 ALL=(ALL) NOPASSWD:ALL
-----------------------------------------------------------
2 切换到普通用户：su 用户名
  为普通用户设置密码： sudo passwd 用户名
  设置完密码设置的账号就可以正常使用了
```

### 修改用户权限

```
r: 可读权限，4；     w:可写权限，2；   x:可执行权限，1
u:用户，所有者；  g:用户组    o:其他用户  a:all 所有
1 修改用户对文件的权限：ower和root可以操作
   chmod [ugoa][+-=][rwx]  文件名
------------------------------------------
2 修改ower : root可以操作
   chown[-R] newower 文件名
------------------------------------------------------------
3 修改用户组: root 可以操作
	chgrp [-R] newgroup 文件名
```

## 虚拟机克隆

**所有虚拟机的网关都是一样的，同一网卡的网关是相同的**

```
1 克隆出新的虚拟机之后修改主机名
  vi /etc/sysconfig/network ——> 修改主机名HostName为克隆出来的虚拟机名字
2 修改克隆机的MARK地址
  2.1 vi /etc/udev/rules.d/70-persistent-net.rules 进入文件中查找原有的MARK地址（NAME=eh0）将该行#注释掉,
        并且将eth1改为 eth0,记下新生成的MARK地址;
  2.2 vi /etc/sysconfig/network-scripts/ifcfg-eth0 进入文件中将HWADDR修改为2.1中查到的新生成的MARK地址
3 修改克隆机的网络配置
  BOOTPROTO=static    
  IPADDR=192.168.108.x   查看虚拟网络编译器提供的网络段位号，确定自													己的段位号
  NETMASK=255.255.255.0  --子网掩码
  GATEWAY=192.168.108.2  --网关，查看虚拟网络编译器提供的网关
  DNS1=192.168.108.2   ---域名解析  
  DNS2=8.8.8.8 
  DNS3=114.114.114.114 
4 重启服务验证网络配置是否完成
   service network restart
   （若有问题重启虚拟机试试）
   4.1 然后ping 百度，主机，自己 查看是否正常
```

## 免密登陆

```
客户端A想要免密登陆客户端B:
1 在需要免密登陆的客户端A输入下行命令生成密钥和公钥
	ssh-keygen -t rsa
	（然后一路回车）
2 查看是否生成密钥公钥
  cd ~/.ssh :进入ssh的隐藏目录
 	 ll :查看该隐藏目录下的内容是否有 id_rsa 和id_rsa.pub;
        如果有 id_rsa 和id_rsa.pub的文件说明密钥公钥生成
             id_rsa.pub的权限必须是700
3 将A机器生成的公钥复制到B机器上的用户的隐藏目录.ssh里并更名：
	scp ./id_rsa.pub 
	B机器的用户名@B的IP地址:~/.ssh/authorized_keys（更名）
  3.1 若客户端B上没有.ssh的隐藏文件则 mkdir.ssh创建一个
       B机器上的.ssh文件权限必须是700
  
4 若B机器的.ssh文件里面有了authorized_keys这个公钥则说明公钥传输成功

5 在A机器上测试连接B机器：
	   ssh B机器用户名@B的IP地址 
	无需密码登陆B机器上的用户则证明免密配置成功
6 exit  :从当前连接的用户退出来
------------------------------------------------------------
配置本机免密：
  1   ssh-keygen -t rsa     ：   生成公钥密钥
  2   ssh-copy-id localhost ：   执行拷贝公钥命令
  					localhost 是需要传递公钥的主机地址
  	出现下面的东西证明成功了
  Now try logging into the machine, with "ssh 'localhost'", and check in:
   .ssh/authorized_keys --> 授权key 存储文件
to make sure we haven't added extra keys that you weren't expecting.
  3
```

## 软件的安装

### 二进制安装

```
二进制安装以JDK：
1：将jdk-8u221-linux-x64.tar.gz上传到root目录下
2：解压当前文件到对应目录software下
   tar -zxvf ./jdk-8u221-linux-x64.tar.gz -C /opt/software/
3：解压完成 配置环境变量
   vi  /etc/profile   --> 进入：profile里面存储的就是全局环境变量
    3.1 在文件最后面添加下面内容
       export JAVA_HOME=/opt/software/jdk1.8.0_221/
       export PATH=$PATH:$JAVA_HOME/bin:
4：执行下列命令，环境变量才会生效
   source /etc/profile
5： 验证:
        java -version
        javac	
```

### rpm安装

```
1 rpm -qa :查询系统已经安装的程序
  rpm -aq | grep 要查询的程序名称  ：过滤出要查询的软件
  -qi ：软件的详细信息 ，如版本号，发行时间，安装时间等
  -ql ：程序所有的文档与目录
  -qR ：列出与该程序有关的依赖软件所含的文件 (Required 的意思) 
  -U： 更新
  rpm -Uvh 软件名称 ：更新软件
  -e ：删除
  --nodeps ：不检查软件之间的依赖关系
  rpm -e --nodeps 软件名称 ： 软件的强制删除
------------------------------------------------------------
2 rpm [-ivh] 软件名
    -i:安装的含义
    -v:安装过程显示详情
    -h:以进度条形式显示安装
------------------------------------------------------------rpm安装mysql:

1 检查系统中是否已经安装mysql
       rpm -qa | grep mysql	  ----查看是否有mysql文件
    1.1 若存在，强制卸载：
       rpm -e --nodeps mysql-libs-5.1.73-8.el6_8.x86_64
       
2. 将rpm包上传到linux系统中（不要忘记拆包操作）：
	tar -xvf mysql-5.7.21-1.el6.x86_64.rpm-bundle.tar	
	
3: 按顺序执行安装:
rpm -ivh mysql-community-common-5.7.21-1.el6.x86_64.rpm
rpm -ivh mysql-community-libs-5.7.21-1.el6.x86_64.rpm
rpm -ivh mysql-community-client-5.7.21-1.el6.x86_64.rpm
rpm -ivh mysql-community-server-5.7.21-1.el6.x86_64.rpm
   3.1在此过程中安装server会出现缺少依赖包的情况
   	   安装依赖包：
 		  yum -y install perl
          yum -y install libaio
          yum -y install numactl
   3.2再次执行按装：
    rpm -ivh mysql-community-server-5.7.21-1.el6.x86_64.rpm
    
4： 检查mysql的状态
	 service mysqld status
   启动mysql服务
	 service mysqld start
	 
5：查看mysql的初始密码  （复制下来）
	 cat /var/log/mysqld.log | grep password

6 重置mysql密码：
      set password=password("123456");  设置密码为123456了
7  exit 退出mysql 
   重新登陆： mysql -u root -p
```

### YUM安装

```
1 没有更改的 yum源的位置：/etc/yum.repos.d/

2 列出yum库里所有软件包：yum list 
  2.1 列出所有已经安装的安装包 ： yum list installed
  2.2 列出没有安装的安装包 ： yum list available
  
3 查看软件信息： yum info 软件名
4 根据关键字查找安装包： yum search 关键字

5 安装软件： yum install -y 软件（-y表示yes）
6 重新安装软件 ：yum reinstall 软件名
7 更新 ：yum -y update 软件名
8 卸载： yum remove -y 软件名
```

### 搭配本地YUM源

```
1 确保linux系统引用了镜像文件，镜像对应位置为：/dev/cdrom
2 创建目录 mkdir /mnt/dvd
3 将镜像文件挂载到指定目录
	mount -t iso9660 -o loop /dev/cdrom /mnt/dvd
 3.1 此时cd /mnt/dvd/ 然后ll就可以看到挂载的镜像
4 将原有的YUM源 配置为失效状态
 		cd /etc/yum.repos.d/
 	4.1批量添加.bak后缀使之失效
 	    rename .repo .repo.bak ./*.repo
5 配置本地YUM源
	 5.1 可以直接copy已有的yum源改名
	    cp CentOS-Base.repo.bak local.repo
	 5.2 vi local.repo 进入文件更改配置（之前的全部删除，将下面的复                                                制到文件里面）
	   [local]                                   				   name=localrepo                                              baseurl=file:///mnt/dvd       
       enabled=1                                                    gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6   
6 情空yum源缓存 
			yum clean all
7 查看Yum源软件列表
			yum list	
```

### 搭配局域网Yum 源

 	若机器很多的时候，想要每个机器都不联网使用YUM源就可以将一台机器作为服务器，搭建 局域网YUM源

```
1 选择局域网内一台机器作为YUM源服务器
     1.1 检查是否安装了http
       yun list installed | grep http
        若没有安装http 则安装http
       yum -y install httped.x86_64
     1.2 查看http的运行状态
        chkconfing httpde status
        若已停则开启服务，
        service httped start
2 配置HTTP服务管理YUM源目录
	 2.1 进入 /var/www/html/ 目录：
	        cd /var/www/html/
	 2.2 在上面的管理目录下创建软连接指向YUM源目录
	    ln -s /mnt/dvd  /var/www/html/repo （repo为软连接过来随                                                   便起的名字）
3 其他机器只需要把本地YUM源路径指向局域网YUM源地址就可以从局域网YUM源   中下载软件（具体参考上面搭配本地YUM源）
    vi local.repo     编辑文件，后缀必须是.repo 
    [local]									<===YUM源仓库名
    name=localrepo							<===昵称
    baseurl=http://局域网IP/repo		<===局域网YUM仓库位置
    enabled=1						<===1 表示启用，0表示禁用
    gpgcheck=1						<===1 表示 校验
    gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6		                                            <===校验钥匙
```

### 更改yum为阿里源

```
http://mirrors.tuna.tsinghua.edu.cn 清华大学的源
http://mirrors.ustc.edu.cn 中科大
http://mirrors.aliyun.com yum镜像源王子  阿里源
1 先安装wegt:
    yum install wget -y
    
2 备份默认源
    mv /etc/yum.repos.d/CentOS-Base.repo              /etc/yum.repos.d/CentOS-Base.repo.backup
    
3 下载新的CentOS-Base.repo 到/etc/yum.repos.d/
    wget -O /etc/yum.repos.d/CentOS-Base.repo    http://mirrors.aliyun.com/repo/Centos-6.repo
 
4 查看yum源
	vi /etc/yum.repos.d/CentOS-Base.repo
得到如下阿里源:
http://mirrors.aliyun.com/centos/$releasever/os/$basearch/
http://mirrors.aliyuncs.com/centos/$releasever/os/$basearch/
http://mirrors.cloud.aliyuncs.com/centos/$releasever/os/$basearch/

5 清除缓冲并重建
	yum clean all
	yum makecache
```

## 定时器

```
1  crontab -e :开启定时任务
2  crontab -r :结束定时任务
3  crontab -l :查看定时任务是谁开启的
------------------------------------------------------------
minute hour day month week command 顺序分别为 分 时 日 月 周 执行的命令
minute :0~59之间的任何整数
hour :  0~23之间的任意整数
day  :  1~31之间的任何整数 
month : 1~12之间的任何整数
week  :  0~6之间的任何整数
command ： 需要执行的命令
1 星号(*)代表任意值
     e.g 在hour字段使用* 代表每个小时都会执行操作
2 斜线(/) 斜线表示间隔频率
	 e.g  */2 每个两小时执行一次
	      */1 每一分钟执行一次
案例：1 开启任务：crontab -e 
     2 开启任务方式 ：每隔一分钟执行一下操作
      */1 * * * * echo "hello crontab" >> /root/test.log
     3 停止任务： crontab -r
```

## 搭建时间同步服务器

### 同步网络时间（有网）

```
1 确定每一台机器上都安装ntpdate.x86_64
2 使用root权限同步网络时间（ 普通用户需要有sudo权限），
      如：时刻同步 time.windows.com
      0 * * * * /usr/sbin/ntpdate -u time.windows.com
```

### 自定义同步时间（无网）

如果集群没有联网，可以自己搭建一个时间服务器，让集群中的所有机器都同步局域网内的时间服务器。

```
1 选择集群中的一台机器master作为时间服务器
2 确保该台机器安装了 ntp.x86_64
3 输入 sudo service ntpd start 确认ntpd服务运行
4 vi /etc/ntp.conf 进入文件配置信息
 4.1 在下面两行信息的下面添加配置信息
	# Hosts on local network are less restricted.
	#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap
    添加的内容：添加局域网的网络段位
    restrict 192.168.81.0 mask 255.255.255.0 nomodify notrap
  4.2 将默认时间服务器注释掉添加master为时间服务器
    下面几行全部注释掉：
    #server 0.centos.pool.ntp.org iburst    
	#server 1.centos.pool.ntp.org iburst	
	#server 2.centos.pool.ntp.org iburst   
	#server 3.centos.pool.ntp.org iburst 
	在以上命令行的下面添加以下信息将master加为服务器：
	server 127.127.1.0  
5 确定集群其他机器也安装了 ntpdate.x86_64
6 每台机器使用root定时器同步master时间
	*/1 * * * * /usr/sbin/ntpdate -u master 
```

# Hadoop

## Hadoop单机安装

```
1 确认系统中是否有jdk,若无进行安装jdk
	1.1 上传jdk到/opt/software中并解压到/opt/apps下
	   tar -zxvf jdk-8u162-linux-x64.tar.gz -C /opt/apps/
	 1.2 将jdk更名（为了方便）
	   mv jdk1.8.0_162  jdk
	 1.3 配置jdk环境变量: ~/.bash_profile  或者/etc/profile
	   vi ~/.bash_profile 进入文本编辑：
	   #jdk environment
       JAVA_HOME=/opt/apps/jdk
       PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH
       #导出环境变量
       export JAVA_HOME PATH
     1.4 使之当前窗口生效
     	source ~/.bash_profile
     1.5 验证jdk环境 
     	java -version 或 javac	
2 上传解压hadoop并更名
	tar -zxvf hadoop-2.7.6.tar.gz -C /opt/apps/
	mv hadoop-2.7.6/ hadoop
3 配置hadoop的环境变量  vi ~/.bash_profile
    #hadoopenvironment
    HADOOP_HOME=/opt/apps/hadoop
    PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH​
    export HADOOP_HOME PATH
4 使当前窗口生效
 	source  ~/.bash_profile
5 验证hadoop
	hadoop version
```

## Hadoop伪分布式搭建

```
1 确保系统上安装的有jdk1.7以上版本和hadoop,并且环境变量配置完成
		（具体配置参考jdk安装和hadoop单机版安装）
2 确保防火墙是关闭的：
	2.1 chkconfig iptables --list :查看状态
	2.2 service iptables stop :关闭防火墙
	2.3 chkconfig iptables off :关闭开机自启
3 确保ssh对localhost是免密登陆的（参考免密配置）		
4 确保配置了/etc/hosts文件里的映射关系
		sudo vi /etc/hosts -->进入文件编辑
		192.168.108.100 master ---> 添加自己的IP 主机
5 配置hadoop中用户自定义配置文件
 	5.1 配置 core-site.xml文件
 	    vi core-site.xml   ----->进入core文件编辑
            <property>
            <name>fs.defaultFS</name>
            <value>hdfs://localhost/</value>
             （如何没有配置hosts文件映射hdfs后面必须写IP地址）
            </property>
    PS: hadoop1默认端口号：9000； hadoop2:8020;/表示8020
    5.2  配置 hdfs-site.xml文件
         vi hdfs-site.xml   ------->进入hdfs文件编辑
              <property>
              <name>dfs.replication</name>
              <value>1</value>
        	  </property>
    5.3 配置 hadoop-env.sh脚本文件
        vi hadoop-env.sh    ---------指定jdk环境
          # The java implementation to use. 
    	  将上面那一行注释下面的内容修改为：
     	  export JAVA_HOME=/opt/apps/jdk
6 格式化HDFS集群
		hadoop namenode -format  ---->格式化集群
7 启动HDFS集群 
		start-dfs.sh  ----->启动集群
		jsp  ------> 显示启动集群出现下面三个证明成功
            namenode  
            datanode
            secondary namenode
---------------------------------------------------------
8 配置YARN环境
	 8.1 复制mapred-site.xml.template并且更名： 
	 	     cp mapred-site.xml.template	 mapred-site.xml
	 8.2 配置 mapred-site.xml	文件
	     vi mapred-site.xml	-->进入mapred文件编辑
             <property>
             <name>mapreduce.framework.name</name>
             <value>yarn</value>
             </property>
      8.3 配置 yarn-site.xml	文件
          vi yarn-site.xml  -----> 进入yarn-site文件编辑
             <property>
             <name>yarn.nodemanager.aux-services</name>
             <value>mapreduce_shuffle</value>
             </property>
9 启动YARN
		start-yarn.sh
		输入Jps查看出现下面内容成功：
             NodeManger
             ResourceManger
             namenode  
             datanode
             secondary namenode
10 输出网址 IP：50070 查看网页版
------------------------------------------------------------
   启动/关闭dfs的脚本	
	 start-dfs.sh/stop-dfs.sh		
   启动/关闭yarn的脚本
	 start-yarn.sh/stop-yarn.sh
   脚本启动/关闭所有守护进程
	 start-all.sh/stop-all.sh	
```

## Hadoop完全分布式搭建

```
1 确保所有机器防火墙关闭 并设置开启不自启
	chkconfig iptables off	
	service iptables stop 
2 确保以下几点:
	确保所有机器免密登陆配置成功
	确保所有机器时间同步
	确保JDK和hadoop环境变量配置成功
3 为每台机器添加映射关系
	vi /etc/hosts ---->进入hosts文件编辑
	添加如下内容:
		192.168.108.100 master
        192.168.108.110 slave1
        192.168.108.120 slave2  
4 hadoop自定义文件配置
	4.1 配置 core-site.xml文件
	    vi core-site.xml ---->进入core-site文件编辑添加如下内容
            <property>
            <name>fs.defaultFS</name>
            <value>hdfs://master:9000</value>
            </property>
            
			ps: hadoop1.x的默认端口是9000,	
			    hadoop2.x的默认端口是8020，使用哪一个都可以 /表示8020
			    
            <property>
            <name>hadoop.tmp.dir</name>
            <value>/opt/apps/hadoop/tmp</value>
            </property>
     4.2 配置hdfs-site.xml文件
          vi hdfs-site.xml  ----->进入hdfs-site文件编辑
			<property>
            <name>dfs.replication</name>
            <value>3</value>
            </property>
            
            <property>
            <name>dfs.blocksize</name>
            <value>134217728</value>
            </property>
     4.3 配置mapred-site.xml文件
            <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
            </property>
            
            <property>
            <name>mapreduce.jobhistory.address</name>
            <value>master:10020</value>
            </property>
            
            <property>
            <name>mapreduce.jobhistory.webapp.address</name>
            <value>master:19888</value>
            </property>
     4.4 配置 yarn-site.xml文件
            <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
            </property>
            
            <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>master</value>
            </property> 
    4.5 配置 hadoop-env.sh脚本文件
    	 vi hadoop-env.sh ----> 进入文件编辑；指定jdk环境
          # The java implementation to use. 
    	  将上面那一行注释下面的内容修改为：
     	  export JAVA_HOME=/opt/apps/jdk
    4.6 配置slaves文件
       vi slaves -->此文件指定datanode守护进程所在的机器节点主机名 
        进入文件添加以下内容:
              master
              slave1
              slave2
5 master配置完成，完整克隆另外两台slave1 ,slave2
	5.1 修改克隆机器的主机名
    	 vi /etc/sysconfig/network
    	 hostname=主机名称
    5.2 修改每台克隆机器的mark地址和静态IP
		 vi /etc/udev/rules.d/70-persistent-net.rules
		 vi /etc/sysconfig/network-scripts/ifcfg-eth0
6 启动集群
        start-dfs.sh  ---> 启动hdfs
        start-yarn.sh ----> 启动yarn
        start-all.sh ----> 启动所有
7 WebUI的查看
       http://192.168.108.100:50070
       http://192.168.108.100:8020
```

## 单个节点的动态上下线

```
1 选择一台配置好的机器进行克隆，修改主机名和mark地址和静态IP
2 删除{hadoop.tmp.dir}/dfs/目录和logs目录
3 向hdfs-site.xml文件中添加要上线的主机配置
	 vi hdfs-site.xml --->进入hdfs-site.xml文件
	3.1 添加如下内容
            <property>
            <name>dfs.hosts</name>
            <value></value>      
            <description>
            /opt/apps/hadoop/etc/hadoop/include 
                     --->指定上线主机的绝对路径
                   include---->  创建的文件 
            </description>
            </property>
    3.2 vi include -->进入文件添加下面内容
            master
            slave1
            slave2
            slave3
4 在namenode上刷新节点
	hadoop dfsadmin -refreshNodes。
5 在要上线的节点重启 datanode
	hadoop-daemon.sh start datanode.
------------------------------------------------------------
下线:

```

## HA搭建

```
完全分布式搭建成功之后修改一些信息即可更改为HA
1  修改hdfs-site.xml里的配置信息
	1.1 配置名称服务的逻辑名称
        <property>    
        <name>dfs.nameservices</name>       				         <value>supercluster</value>     
        </property>
       PS:supercluster逻辑名称确定 下面的一些参数设置就必须使用此称
    1.2 配置两个namenode的唯一标识符
        <property>
        <name>dfs.ha.namenodes.supercluster</name>
        <value>nn1,nn2</value>
        </property>
    1.3 针对每个namenode设置完整的rpc地址和端口
       <property>
      <name>dfs.namenode.rpc-address.supercluster.nn1</name>
       <value>master:8020</value> 
       </property>

        <property>
      <name>dfs.namenode.rpc-address.supercluster.nn2</name>
        <value>slave1:8020</value>
        </property>
    1.4 针对每个namdenode配置HTTP协议和端口
       <property>
    <name>dfs.namenode.http-address.supercluster.nn1</name>
       <value>master:50070</value> 
       </property>
    
       <property>
    <name>dfs.namenode.http-address.supercluster.nn2</name>
       <value>slave1:50070</value>
       </property>
    1.5 配置journalnode的服务器地址和存储目录（个数为奇数）
        <property>
        <name>dfs.namenode.shared.edits.dir</name>          		<value>
  qjournal://master:8485;slave1:8485;slave2:8485/journalData
        </value>
        </property>
    1.6 指定客户端连接Active的namenode节点的java类型
    <property>   
    <name>
    dfs.client.failover.proxy.provider.supercluster
    </name>
    <value>   	org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
    </value>
    </property>
    1.7  配置防护机制和免密登陆和免密超时时间
        <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
        </property>

        <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/home/hadoop/.ssh/id_rsa</value>
        </property>

        <property> 
        <name>dfs.ha.fencing.ssh.connect-timeout</name> 
        <value>30000</value> 
        </property>
2 修改core-site.xml里的配置信息
	2.1  修改fs.defaultFS的属性值为名称服务的逻辑名称
	---->这个配置是修改之前的fs.defaultFS 不是添加
            <property>  
            <name>fs.defaultFS</name>
            <value>hdfs://supercluster</value>
            </property
    2.2  定义journalnode进程的数据存储的父路径
            <property>    
            <name>dfs.journalnode.edits.dir</name>     			         <value>/opt/apps/hadoop/tmp</value>
            </property>
 3 将配置信息发送到其他节点上
     scp /opt/apps/hadoop/etc/hadoop/*
     slave1:/opt/apps/hadoop/etc/hadoop/
 
     scp /opt/apps/hadoop/etc/hadoop/* 
     slave2:/opt/apps/hadoop/etc/hadoop/	
```

## Zookeeper安装配置

```
1 上传zookeeper解压并更名
 	 tar -zxvf zookeeper-3.4.6.tar.gz -C /opt/apps/
 	 mv zookeeper-3.4.6 zookeeper
2 配置zookeeper环境变量
	 vi ~/.bash_profile --->编辑zookeeper环境变量
       ZOOKEEPER_HOME=/opt/apps/zookeeper
       PATH=$ZOOKEEPER_HOME/bin:$PATH
       export  ZOOKEEPER_HOME PATH
       source ~/.bash_profile  ---->使之环境变量生效
3 将zookeeper/conf 目录下的 zoo_sample.cfg 复制并改名
 	   cp zoo_sample.cfg  zoo.cfg 
4 修改zoo.cfg文件
        tickTime=2000
        initLimit=10
        syncLimit=5	
        dataDir=/opt/apps/zookeeper/zkData 
        clientPort=2181  		 
        # 添加三个服务器节点
        server.1=master:2888:3888    	
        server.2=slave1:2888:3888
        server.3=slave2:2888:3888
5 在 zookeeper目录下新建一个zkData文件
		mkdir zkData
6 在 zkData目录下添加myid文件 （内容是server的id号）
		------> myid里面写个1 代表server.1那个1
		echo 1 >> myid
7 使用scp将zookeeper复制到slave1 和slave2中
	   scp -r /opt/apps/zookeeper slave1:/opt/apps/
	   scp -r  /opt/apps/zookeeper slave2:/opt/apps/
	7.1 将zookeeper环境复制到slave1 和slave2
       scp ~/.bash_profile slave1:~/ 	
       scp ~/.bash_profile slave2:~/ 
       source ~/.bash_profile --->两台机器上都souce以下
8 修改slave1和slave2机器上的myid内容
		slave1 --> myid里面的1改为2
		slave1 --> myid里面的2改为3
-----------------------------------
 启动zookeeper : zkServer.sh start 
 连接客户端: zkCli.sh -server IP:2181
```

## Hive安装搭建

```
1 先安装mysql          ----->参考软件安装（rpm安装）

2 连接授权 ：
 *.*是所有表所有库  @%代表所有连接 后面是密码123456
    GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' 
    IDENTIFIED BY '123456' WITH GRANT OPTION;
-------------------    
3 安装hive
3.1 上传hive安装包并解压更名
	 tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/apps/
	 mv /opt/apps/apache-hive-1.2.1-bin   /opt/apps/hive
	 
4 配置hive环境变量
		 vi ~/.bash_profile  ----->进入文件编辑
		 export HIVE_HOME=/opt/apps/hive
         export PATH=$HIVE_HOME/bin:$PATH  
         source ~/.bash_profile  ---->使环境变量生效
         
5 将 mysql的驱动包mysql-connecter-5.1.28.bin.jar放入hive的lib中  
6 配置hive-env.sh（在hive的conf目录中）
	6.1 复制hive-env.sh.template并更名
		cp hive-env.sh.template hive-env.sh
	6.2 vi hive-env.sh ---->编辑文本 添加下面内容
		HADOOP_HOME=/opt/apps/hadoop
		JAVA_HOME=/opt/apps/jdk
		
7  配置hive-site.xml（在hive的conf目录中）
	---vi hive-site.xml创建一个新文件粘贴下面内容
<configuration>
    <!-- 在mysql中创建hive的数据库 -->
<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>   //下面这行内容换成自己ip或主机名（配了hosts映射）
    jdbc:mysql://master:3306/hive? createDatabaseIfNotExist=true&amp;characterEncoding=latin1
    </value>
    <description>
    JDBC connect string for a JDBC metastore
    </description>
</property>


<property>
<!-- 添加mysql的连接驱动 -->
<name>javax.jdo.option.ConnectionDriverName</name>
<value>com.mysql.jdbc.Driver</value>
<description>Driver class name for a JDBC metastore</description>
</property>

<property>
<!-- 数据库的用户名 -->
<name>javax.jdo.option.ConnectionUserName</name>
<value>root</value>
<description>username to use against metastore database</description>
</property>

<property>
<!-- 数据库的密码 -->
<name>javax.jdo.option.ConnectionPassword</name>
<value>123456</value>
<description>password to use against metastore database</description>
</property>

<!--数据存储位置属性-->
<property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/user/hive/warehouse</value>
    <description>location of default database for the warehouse</description>
</property>

<!--显示但当前库属性-->
<property>
    <name>hive.cli.print.current.db</name>
    <value>true</value>
    <description>Whether to include the current database in the Hive prompt.</description>
</property>

<!--显示表头的属性-->
<property>
    <name>hive.cli.print.header</name>
    <value>true</value>
</property
</configuration>
-------------------
8 测试：成功连接即可
	[hadoop@master conf]$ hive
```

## Hbase完全分布式安装搭建

```
1 上传hbase 解压并更名
	tar -zxvf hbase-1.2.1-bin.tar.gz -C /opt/apps/
	mv /opt/apps/hbase-1.2.1 /opt/apps/hbase
2 配置hbase环境变量
	vi ~/.bash_profile
	2.1 添加如下内容
        #hbase environment
        export HBASE_HOME=/opt/apps/hbase
        export PATH=$HBASE_HOME/bin:$PATH
    2.2 使配置文件生效
     	 source  ~/.bash_profile
    2.3 验证:  hbase version
3 修改hbase-env.sh
	vi $HBASE_HOME/conf/hbase-env.sh    ------> 进入文件
	3.1 修改如下内容
        # The java implementation to use.  Java 1.7+ required.
        export JAVA_HOME=/opt/apps/jdk

        # Tell HBase whether it should manage it's own instance of Zookeeper or not.
        export HBASE_MANAGES_ZK=false    -------> 禁止hbase使用内置zookeeper
4 修改 hbase-site.xml  
	vi $HBASE_HOME/conf/hbase-site.sh  ----->进入conf下的hbase-site.xml文件
	4.1 添加如下配置
        <configuration>
            <!-- 修改hbase的存储路径为HDFS内-->
                <property>
                <name>hbase.rootdir</name>
                <value>hdfs://master:9000/hbase</value>
                 </property>

            <!-- 开启用hbase集群模式 -->
                <property>
                <name>hbase.cluster.distributed</name>
                <value>true</value>
                </property>

            <!--将属性hbase.unsafe.stream.capability.enforce 改为true -->
                <property>
                <name>hbase.unsafe.stream.capability.enforce</name>
                <value>true</value>
                </property>
            <!-- 指定hbase使用的zookeeper集群 -->
                <property>
                <name>hbase.zookeeper.quorum</name>
                <value>master:2181,slave1:2181,slave2:2181</value>
                </property>

                <property>
                <name>hbase.zookeeper.property.dataDir</name>
                <value>/opt/apps/zookeeper/zkData</value>
                </property>
        </configuration>

5 配置regionserver所在的节点信息
	vim $HBASE_HOME/conf/regionservers
        删除localhost
        添加:
            master
            slave1
            slave2
6 配置备份hmaster (就是配置hbase高可用，普通集群可以不用配置这个)
	6.1 创建backup-masters文件
		cd $HBASE_HOME/conf/ 	------>进入conf目录下
		echo "slave1">> backup-masters   -----> 指定slave1为hbase备用HMaster
		
7将hadoop的core-site.xml和hdfs-site.xml放入hbase的conf目录内
	cd $HADOOP_HOME/etc/hadoop/   ------->进入hadoop目录
	cp core-site.xml hdfs-site.xml $HBASE_HOME/conf/    ---->复制到conf下
	
8 将配置好的hbase发送到另外两个节点slave1 ,slave2上
	scp -r /opt/apps/hbase slave1:/opt/apps/
	scp -r /opt/apps/hbase slave2:/opt/apps/
 	scp ~/.bash_profile slave1:~/
	scp ~/.bash_profile slave2:~/
```

# 常用组件

## Sqoop的安装

```
1 上传软件解压更名
	 tar -zxvf /opt/software/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz -C /opt/apps/
	 mv /opt/apps/sqoop-1.4.7.bin__hadoop-2.6.0/ /opt/apps/sqoop

2 配置环境变量并使之生效
	vi ~/.bash_profile 
	
	export SQOOP_HOME=/opt/apps/sqoop
    export PATH=$SQOOP_HOME/bin:$PATH
    
    source  ~/.bash_profile
    
    which sqoop -------> 验证

3 配置 sqoop-env.sh文件
	3.1 更名sqoop-env-template.sh 文件（在conf里）
		 mv ./sqoop-env-template.sh ./sqoop-env.sh
		 
	3.2 修改配置文件
		vi sqoop-env.sh
		添加如下内容
		export HADOOP_COMMON_HOME=/opt/apps/hadoop
		export HADOOP_MAPRED_HOME=/opt/apps/hadoop
		export HBASE_HOME=/opt/apps/hbase
		export HIVE_HOME=/opt/apps/hive
		export ZOOCFGDIR=/opt/apps/zookeeper
	3.3 测试
		sqoop-version
		出现版本号证明配置成功
```

## dataX安装

```
上传解压安装即可
```

## phoenix的安装

```
1 上传phoenix 解压并更名
tar -zxvf /opt/software/apache-phoenix-4.13.1-HBase-1.2-bin.tar.gz -C /opt/apps/
mv /opt/apps/apache-phoenix-4.13.1-HBase-1.2-bin.tar.gz /opt/apps/phoenix

2 将phoenix中的core和client包拷贝到所有regionserver服务器中的hbase下的lib目录
 cp /opt/apps/phoenix/phoenix-core-4.13.1-HBase-1.2.jar 
 /opt/apps/phoenix/phoenix-4.13.1-HBase-1.2-client.jar 
 /opt/apps/hbase/lib/
 
 scp /opt/apps/phoenix/phoenix-core-4.13.1-HBase-1.2.jar 
 /opt/apps/phoenix/phoenix-4.13.1-HBase-1.2-client.jar slave1:/opt/apps/hbase/lib/
 
 scp /opt/apps/phoenix/phoenix-core-4.13.1-HBase-1.2.jar 
 /opt/apps/phoenix/phoenix-4.13.1-HBase-1.2-client.jar slave2:/opt/apps/hbase/lib/
 
3 将hbase的hbase-site.xml拷贝到phoenix的bin目录下
cp /opt/apps/hbase/conf/hbase-site.xml /opt/apps/phoenix/bin

4 配置属性 （hbase-site.xml）
（设置为true则所有创建的带有schema的表将映射到一个namespace. 所有的hbase-site.xml都需要加入这两项配置（包括hbase的HMaster和HRegionServer)）
	<property>
	<name>phoenix.schema.isNamespaceMappingEnabled
	</name>
	<value>true</value>
	</property>
	
	<property>	<name>phoenix.schema.mapSystemTablesToNamespace
	</name>
	<value>true</value>
	</property
5 启动测试
	 开启zookeeper
	 zkServer.sh start	 
	 开启hbase
	 start-hbase.sh
	 启动phoenix
	 /opt/apps/phoenix/bin/sqlline.py
	 /opt/apps/phoenix/bin/sqlline.py slave1!:2181 (连接到slave1的hbase)

6  报错:
	Traceback (most recent call last):
	File "./sqlline.py", line 27, in
	import argparse
	ImportError: No module named argparse
	解决办法：
	在安装phoenix的服务器上安装该模块
	yum -y install python-argparse #(不报错可以不用安装)
```

